{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c36b7d",
   "metadata": {},
   "source": [
    "# Uso de Técnicas Multivariantes en Datos sobre Enfermedades Coronarias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b55b0",
   "metadata": {},
   "source": [
    "#### La cardiopatÍa isquémica y otras enfermedades cardiovasculares son una de las principales causas de muerte a nivel global. Este trabajo parte de una base de datos real recogida en Framingham de más de 4000 registros y 16 variables. La variable respuesta es binaria y representa si el individuo del que se han obtenido los datos sufrirá cardiopatía isquémica en los 10 años siguientes. Se utilizan técnicas multivariantes no supervisadas (PCA y Clustering), supervisadas (árboles de decisión y SVM) y regresión logit para predecir la variable respuesta. Otro objetivo principal es comparar los modelos por sexo. Las conclusiones del estudio son la mejor predicción para las mujeres y unos resultados similares para todos los algoritmos realizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533252bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4db17",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac51dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan los datos y se le añade una columna con constante\n",
    "# para la regresión que se utilizará más adelante.\n",
    "df=pd.read_csv(\"framingham.csv\")\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "col = list(df.columns)\n",
    "col2= ['constante', col]\n",
    "columns_names = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se muestran los histogramas de las variables a estudiar.\n",
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows,cols,i+1)\n",
    "        dataframe[feature].hist(bins=20,ax=ax,facecolor='royalblue')\n",
    "        ax.set_title(feature+\" Distribution\", color='black')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "draw_histograms(df, df.columns, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1db29b",
   "metadata": {},
   "source": [
    "#### Se ven muchas variables dummies, entre ellas la variable objetivo.  Sobre esta variable se volverá tras el estudio descriptivo, puesto que tiene demasiados valores nulos respecto a los positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fb876",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Como los datos tienen escalas muy diferentes, se escalan.\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(df))\n",
    "df_scaled = scaler.transform(df)\n",
    "from statsmodels.tools import add_constant as add_constant\n",
    "df_const = add_constant(df_scaled)\n",
    "df_const = pd.DataFrame(df_const)\n",
    "df_const.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c14d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añaden los nombres de las columnas y se realiza la matriz de correlación.\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "sn.heatmap(df_const.corr(), vmin=-1, vmax=1, annot=True, cmap=\"Blues\", linewidths=.5, ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238ba2b",
   "metadata": {},
   "source": [
    "#### Se va a realizar regresión logística como parte del procesamiento, puesto que dependiendo de la importancia de las variables estas serán candidatas a eliminarse del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chisqprob = lambda chisq, df_const: st.chi2.sf(chisq, df)\n",
    "x = df_const.iloc[:,0:15]\n",
    "y = df.iloc[:,15]\n",
    "x=x.to_numpy()\n",
    "y = y.to_numpy()\n",
    "model=sm.Logit(endog=y, exog=x)\n",
    "result=model.fit()\n",
    "model\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_const = df_const.iloc[:,1:]\n",
    "df_const.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variables significactivas son:\n",
    "df1=df_const[['age','male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\n",
    "x=df1.iloc[:,:-1]\n",
    "y=df1.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eae6e2",
   "metadata": {},
   "source": [
    "#### La variable sexo es significativa y debido a las diferencias biologics que existen, a partir de ahora se separará la base de datos en hombres y mujeres para hacer los algoritmos por separado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.apply(pd.to_numeric)\n",
    "H = df1[df1['male']>0]\n",
    "H = H.drop(columns='male')\n",
    "M = df1[df1['male']<1]\n",
    "M = M.drop(columns='male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff66cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw=dict(width_ratios=[3, 3]))\n",
    "sn.countplot(x='TenYearCHD',data=H, ax=axs[0])\n",
    "sn.countplot(x='TenYearCHD',data=M,ax=axs[1])\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce2146",
   "metadata": {},
   "source": [
    "#### Se ha podido comprobar que existen demasiados resultados sin enfermedad coronaria para poder realizar un algoritmo de clasificación y que este no discrimine todo como 'NO'.\n",
    "#### Se va a realizar un muestreo aleatorio de los registros sin enfermedad coronaria para crear una nueva base de datos con un número similar de registros negativos y positivos. Esto se realizará cuando se realicen los métodos supervisados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c65ec",
   "metadata": {},
   "source": [
    "## MÉTODOS NO SUPERVISADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c81f7",
   "metadata": {},
   "source": [
    "### ANÁLISIS DE COMPONENTES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65691592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Los dos df 'numer' se corresponden con el original pero el primero solo contiene las var significativas.\n",
    "numer = df[['age','cigsPerDay','BPMeds','totChol','sysBP','diaBP','BMI','heartRate','glucose','TenYearCHD']]\n",
    "# numer = df[['male','age','education','currentSmoker','cigsPerDay','BPMeds','prevalentStroke','prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI','heartRate','glucose','TenYearCHD']]\n",
    "\n",
    "scaler = StandardScaler() # Se va a estandarizar las variables para darle la misma importancia en el modelo.\n",
    "# df = numer.drop(['TenYearCHD'], axis=1) # Se elimina la variable objetivo, solo ejecutar una vez.\n",
    "scaler.fit(numer) # Se calcula la media para poder hacer la transformacion\n",
    "num_scal = scaler.transform(numer)\n",
    "pca = PCA()\n",
    "pca.fit(num_scal)\n",
    "x_pca = pca.transform(num_scal)\n",
    "expl = pca.explained_variance_ratio_\n",
    "print(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tras observar la varianza explicada de los componentes, vemos la misma de los 5 primeros:\n",
    "print('suma:',sum(expl[0:5]))\n",
    "# Gráfico del codo\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Número de componentes')\n",
    "plt.ylabel('Varianza explicada acumulada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico en dos dimensiones, tomando los dos primeros componentes principales\n",
    "Xax=x_pca[:,0]\n",
    "Yax=x_pca[:,1]\n",
    "labels=numer['TenYearCHD'].values\n",
    "cdict={0:'royalblue',1:'orange'}\n",
    "labl={0:'Sin Riesgo',1:'Con Riesgo'}\n",
    "marker={0:'o',1:'o'}\n",
    "alpha={0:.3, 1:.3}\n",
    "fig,ax=plt.subplots(figsize=(7,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "for l in np.unique(labels):\n",
    "    ix=np.where(labels==l)\n",
    "    ax.scatter(Xax[ix],Yax[ix],c=cdict[l],label=labl[l],s=40,marker=marker[l],alpha=alpha[l])\n",
    " \n",
    "plt.xlabel(\"Primer Componente Principal\",fontsize=14)\n",
    "plt.ylabel(\"Segundo Componente Principal\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfcbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de los 3 primeros Componentes principales\n",
    "Zax=x_pca[:,2]\n",
    "df['Zax'] = Zax\n",
    "cdict={0:'royalblue',1:'orange'}\n",
    "labl={0:'Sin Riesgo',1:'Con Riesgo'}\n",
    "y = df.TenYearCHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb70081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar matplotlib y ipympl\n",
    "# %matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "# creating figure\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "for l in np.unique(y):\n",
    " ix=np.where(y==l)\n",
    " ax.scatter(Xax[ix], \n",
    "            Yax[ix], \n",
    "            Zax[ix], \n",
    "            c=cdict[l], \n",
    "            s=12,\n",
    "           label=labl[l])\n",
    "  \n",
    "ax.set_title(\"3D plot\")\n",
    "ax.set_xlabel('Primer Componente')\n",
    "ax.set_ylabel('Segundo Componente')\n",
    "ax.set_zlabel('Tercer Componente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd99d7",
   "metadata": {},
   "source": [
    "### CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9abc6",
   "metadata": {},
   "source": [
    "#### K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "fig = plt.figure()\n",
    "nc = range(1, 30) # El número de iteraciones que queremos hacer.\n",
    "kmeans = [KMeans(n_clusters=i) for i in nc]\n",
    "score = [kmeans[i].fit(numer).score(numer) for i in range(len(kmeans))]\n",
    "score\n",
    "plt.xlabel('Número de clústeres (k)')\n",
    "plt.ylabel('Suma de los errores cuadráticos')\n",
    "plt.plot(nc,score)\n",
    "# Gracias a este método, podemos ver que el número óptimo de conjuntos es 5. Se representará\n",
    "# en 3 dimensiones utilizando PCA y se mostrarán número de conglomerados n=2 y n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203985a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xax=x_pca[:,0]\n",
    "Yax=x_pca[:,1]\n",
    "cdict={0:'royalblue',1:'orange'}\n",
    "labl={0:'Sin Riesgo',1:'Con Riesgo'}\n",
    "from matplotlib import pyplot\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(df_scaled)\n",
    "y = kmeans.labels_\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "for l in np.unique(y):\n",
    " ix=np.where(y==l)\n",
    " ax.scatter(Xax[ix], \n",
    "            Yax[ix], \n",
    "            Zax[ix], \n",
    "            c=cdict[l], \n",
    "            s=12,\n",
    "           label=labl[l])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict={0:'royalblue',1:'orange', 2:'lime', 3:'magenta'}\n",
    "from matplotlib import pyplot\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(df_scaled)\n",
    "y = kmeans.labels_\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "for l in np.unique(y):\n",
    " ix=np.where(y==l)\n",
    " ax.scatter(Xax[ix], \n",
    "            Yax[ix], \n",
    "            Zax[ix], \n",
    "            c=cdict[l], \n",
    "            s=12)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0989eae",
   "metadata": {},
   "source": [
    "#### AGLOMERATIVO\n",
    "\n",
    "#### Este método escoge automáticamente el número de conglomerados óptimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ffa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering().fit(df_scaled)\n",
    "y = clustering.labels_\n",
    "cdict={0:'royalblue',1:'orange', 2:'lime', 3:'magenta'}\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "for l in np.unique(y):\n",
    " ix=np.where(y==l)\n",
    " ax.scatter(Xax[ix], \n",
    "            Yax[ix], \n",
    "            Zax[ix], \n",
    "            c=cdict[l], \n",
    "            s=12)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d840ec3",
   "metadata": {},
   "source": [
    "## REGRESIÓN LOGÍSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se escoge el conjunto de entrenamiento del 70% del total de registros\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc30c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reglog (x_train, y_train, x_test, y_test):\n",
    "    algoritmo = LogisticRegression()\n",
    "    algoritmo.fit(x_train, y_train)\n",
    "    y_pred = algoritmo.predict(x_test)\n",
    "    #print('Summary del modelo')\n",
    "    #print(algoritmo.summary())\n",
    "    #Matriz de Confusión\n",
    "    matriz = confusion_matrix(y_test, y_pred)\n",
    "    print('Matriz de Confusión:')\n",
    "    print(matriz)\n",
    "    #Calculo la precisión del modelo\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precisión del modelo:')\n",
    "    print(precision)\n",
    "    #Calculo la exactitud del modelo\n",
    "    exactitud = accuracy_score(y_test, y_pred)\n",
    "    print('Exactitud del modelo:')\n",
    "    print(exactitud)\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "    especificidad = tn / (tn+fp)\n",
    "    print('Especificidad del modelo')\n",
    "    print(especificidad)\n",
    "    sensibilidad = recall_score(y_test, y_pred)\n",
    "    print('Sensibilidad del modelo:')\n",
    "    print(sensibilidad)\n",
    "    #Calculo el Puntaje F1 del modelo\n",
    "    puntajef1 = f1_score(y_test, y_pred)\n",
    "    print('Puntaje F1 del modelo:')\n",
    "    print(puntajef1)\n",
    "    #Resumen\n",
    "    print(classification_report(y_pred, y_test))\n",
    "    #Calculo la curva ROC - AUC del modelo\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print('Curva ROC - AUC del modelo:')\n",
    "    print(roc_auc)\n",
    "\"\"\"\n",
    "    Gráfico de la curva ROC\n",
    "    y_pred_proba = algoritmo.predict_proba(x_test)[::,1]\n",
    "    from sklearn import metrics\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "reglog(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0ca3f",
   "metadata": {},
   "source": [
    "#### Ya se ha visto que la única variable significativa que es dummy es el sexo. Se ha decidido realizar distintos modelos por sexo. Ahora se realizará por sexos y se equilibrrá el número de casos positivos y negativos en ambos sexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se vrean conjuntos de entrenamiento y de test por sexo\n",
    "Hx=H.iloc[:,:-1]\n",
    "Hy=H.iloc[:,-1]\n",
    "Mx=M.iloc[:,:-1]\n",
    "My=M.iloc[:,-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(Mx,My,test_size=.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se equilibra el número de caoss en ambos sexos\n",
    "shuffled_df_H = H.sample(frac=1,random_state=4)\n",
    "CHD_df_H = shuffled_df_H.loc[shuffled_df_H['TenYearCHD'] == 1]\n",
    "non_CHD_df_H = shuffled_df_H.loc[shuffled_df_H['TenYearCHD'] == 0].sample(n=350,random_state=42)\n",
    "H_equi = pd.concat([CHD_df_H, non_CHD_df_H])\n",
    "H_equi.TenYearCHD.value_counts()\n",
    "sn.countplot(H_equi.TenYearCHD)\n",
    "plt.box(False)\n",
    "plt.xlabel('Enfermedad Coronaria No/Sí',fontsize=11)\n",
    "plt.ylabel('Recuento',fontsize=11)\n",
    "plt.title('Recuento de casos tras equilibrado para Hombres\\n')\n",
    "#plt.savefig('Balance Heart Disease.png')\n",
    "plt.show()\n",
    "\n",
    "H_equi_x=H_equi.iloc[:,:-1]\n",
    "H_equi_y=H_equi.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5312847",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df_M = M.sample(frac=1,random_state=4)\n",
    "CHD_df_M = shuffled_df_M.loc[shuffled_df_M['TenYearCHD'] == 1]\n",
    "non_CHD_df_M = shuffled_df_M.loc[shuffled_df_M['TenYearCHD'] == 0].sample(n=320,random_state=42)\n",
    "M_equi = pd.concat([CHD_df_M, non_CHD_df_M])\n",
    "M_equi.TenYearCHD.value_counts()\n",
    "sn.countplot(M_equi.TenYearCHD)\n",
    "plt.box(False)\n",
    "plt.xlabel('Enfermedad Coronaria No/Sí',fontsize=11)\n",
    "plt.ylabel('Recuento',fontsize=11)\n",
    "plt.title('Recuento de casos tras equilibrado para Mujeres\\n')\n",
    "#plt.savefig('Balance Heart Disease.png')\n",
    "plt.show()\n",
    "\n",
    "M_equi_x = M_equi.iloc[:,:-1]\n",
    "M_equi_y = M_equi.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6559045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los conjuntos de entrenamiento y test equilibrados por sexo.\n",
    "H_equi_x_train, H_equi_x_test, H_equi_y_train, H_equi_y_test = train_test_split(H_equi_x,H_equi_y,test_size=.3,random_state=7)\n",
    "M_equi_x_train, M_equi_x_test, M_equi_y_train, M_equi_y_test = train_test_split(M_equi_x,M_equi_y,test_size=.3,random_state=7)\n",
    "\n",
    "#Tras haber definido la función, se ejecuta con \n",
    "reglog(H_equi_x_train, H_equi_y_train, H_equi_x_test, H_equi_y_test)\n",
    "print(\"\\n --------------------------------------------------------------------------------- \\n\")\n",
    "reglog(M_equi_x_train, M_equi_y_train, M_equi_x_test, M_equi_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d04e",
   "metadata": {},
   "source": [
    "## ANOVA\n",
    "#### Se realizan test ANOVA para diferentes varaibles entre sexos. También se realizan histogramas y boxplot para el estudio de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "# Modelo OLS para distintas variables, en este caso 'totChol'\n",
    "model = ols('totChol ~ C(male)', data=df1).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)\n",
    "\n",
    "sn.set(color_codes =True)\n",
    "sn.distplot(H['sysBP'], rug=True)\n",
    "sn.distplot(M['sysBP'], rug=True)\n",
    "plt.show()\n",
    "\n",
    "data = (H['sysBP'], M['sysBP'])\n",
    "plt.boxplot(data)\n",
    "plt.title(\"Boxplot Hombres y Mujeres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56840fd",
   "metadata": {},
   "source": [
    "## MÉTODOS SUPERVISADOS\n",
    "#### La regresión logística realizada antes se podría considerar ya método supervisado, pero al utilizarla para la separación por sexos se ha decidido utilizarla por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eac7f0",
   "metadata": {},
   "source": [
    "### ÁRBOLES DE DECISIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aee94",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def decision_tree (x_train, y_train, x_test, y_test):\n",
    "        #### RANDOM FOREST ####\n",
    "    algoritmo = RandomForestClassifier()\n",
    "    modelo = RandomForestClassifier(criterion= 'entropy',max_depth=2, random_state=0)\n",
    "    \n",
    "        #### CART ####\n",
    "    #modelo = DecisionTreeClassifier(random_state=7)\n",
    "    #modelo = DecisionTreeClassifier(min_samples_split=40, random_state=7)\n",
    "    #modelo = DecisionTreeClassifier(criterion='gini', max_depth=7)\n",
    "    #modelo = DecisionTreeClassifier(criterion='entropy', max_depth=7)\n",
    "    #Mejora sensibilidad pero empeora especificidad\n",
    "    \n",
    "    modelo.fit(x_train, y_train)\n",
    "    y_pred_dt = modelo.predict(x_test)\n",
    "    y_pred = modelo.predict(x_test)\n",
    "    dt_train_score = modelo.score(x_train, y_train)\n",
    "    print('Clasificación en el cjto de entrenamiento %f' %dt_train_score)\n",
    "    dt_accuracy = accuracy_score(y_pred_dt, y_test)\n",
    "    print('Exactitud del modelo %f'%dt_accuracy)\n",
    "    matriz = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    sn.heatmap(pd.DataFrame(matriz), annot=True, cmap=\"Blues\", ax=ax)\n",
    "    plt.show()\n",
    "    #Calculo la precisión del modelo\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precisión del modelo:')\n",
    "    print(precision)\n",
    "    #Calculo la exactitud del modelo\n",
    "    exactitud = accuracy_score(y_test, y_pred)\n",
    "    print('Exactitud del modelo:')\n",
    "    print(exactitud)\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "    especificidad = tn / (tn+fp)\n",
    "    print('Especificidad del modelo')\n",
    "    print(especificidad)\n",
    "    sensibilidad = recall_score(y_test, y_pred)\n",
    "    print('Sensibilidad del modelo:')\n",
    "    print(sensibilidad)\n",
    "    #Calculo el Puntaje F1 del modelo\n",
    "    puntajef1 = f1_score(y_test, y_pred)\n",
    "    print('Puntaje F1 del modelo:')\n",
    "    print(puntajef1)\n",
    "    print(classification_report(y_pred, y_test))\n",
    "\n",
    "decision_tree(M_equi_x_train, M_equi_y_train, M_equi_x_test, M_equi_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a2d6e",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE\n",
    "#### Para este algoritmo, podemos utilizar la función GirdSearch, que elije los parámetros òptimos del algoritmo para el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b13395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm_clf = GridSearchCV(SVC(kernel='linear', probability=True), param_grid, cv=10)\n",
    "\n",
    "svm_clf.fit(H_equi_x_train, H_equi_y_train)\n",
    "print(\"Para hombres, los mejores parámetros son:\", svm_clf.best_params_ )\n",
    "svm_clf.fit(M_equi_x_train, M_equi_y_train)\n",
    "print(\"Para mujeres, los mejores parámetros son:\", svm_clf.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupportVM (x_train, y_train, x_test, y_test):\n",
    "   # modelo=SVC(kernel='rbf')\n",
    "    modelo=SVC(C=1, kernel='linear', gamma=0.001)\n",
    "    modelo.fit(x_train, y_train)\n",
    "    y_pred_dt = modelo.predict(x_test)\n",
    "    dt_train_score = modelo.score(x_train, y_train)\n",
    "    y_pred = modelo.predict(x_test)\n",
    "    print('Clasificación en el cjto de entrenamiento %f' %dt_train_score)\n",
    "    dt_accuracy = accuracy_score(y_pred_dt, y_test)\n",
    "    matriz = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    sn.heatmap(pd.DataFrame(matriz), annot=True, cmap=\"Blues\", ax=ax)\n",
    "    plt.show()\n",
    "    print('Matriz de Confusión:')\n",
    "    print(matriz)\n",
    "    print('Exactitud del modelo %f'%dt_accuracy)\n",
    "    #Calculo la precisión del modelo\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precisión del modelo:')\n",
    "    print(precision)\n",
    "    #Calculo la exactitud del modelo\n",
    "    exactitud = accuracy_score(y_test, y_pred)\n",
    "    print('Exactitud del modelo:')\n",
    "    print(exactitud)\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "    especificidad = tn / (tn+fp)\n",
    "    print('Especificidad del modelo')\n",
    "    print(especificidad)\n",
    "    sensibilidad = recall_score(y_test, y_pred)\n",
    "    print('Sensibilidad del modelo:')\n",
    "    print(sensibilidad)\n",
    "    #Calculo el Puntaje F1 del modelo\n",
    "    puntajef1 = f1_score(y_test, y_pred)\n",
    "    print('Puntaje F1 del modelo:')\n",
    "    print(puntajef1)\n",
    "    print(classification_report(y_pred, y_test))\n",
    "    #Calculo la curva ROC - AUC del modelo\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print('Curva ROC - AUC del modelo:')\n",
    "    print(roc_auc)\n",
    "    \n",
    "SupportVM(M_equi_x_train, M_equi_y_train, M_equi_x_test, M_equi_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbba7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def split_dataset(X, y, test_size = 0.25, random_state = 1):\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X, y, \n",
    "                                                      test_size = test_size, \n",
    "                                                      random_state = random_state,                                                   shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def draw_matrix_confusion(yTest, yPred, subplot, name, measures):\n",
    "    LABELS = ['No Fraude', 'Fraude'] \n",
    "    conf_matrix = metrics.confusion_matrix(yTest, yPred) \n",
    "    sns.set(font_scale=2)\n",
    "    ax = plt.subplot(3, 2, subplot)\n",
    "\n",
    "    sns.heatmap(conf_matrix, xticklabels = LABELS,  \n",
    "                yticklabels = LABELS, annot = True, fmt =\"d\"); \n",
    "    plt.title(name, fontsize=20, fontweight='bold') \n",
    "    plt.ylabel('Clase verdadera', fontsize=20, fontweight='bold') \n",
    "    plt.xlabel('Clase predicha\\nROC-AUC: {}'.format(measures[-1]), fontsize=20, fontweight='bold') \n",
    "    return conf_matrix\n",
    "\n",
    "# Define las medidas que se van a probar en cada algoritmo\n",
    "def calculate_measures(y_test, yPred):\n",
    "    acc = metrics.accuracy_score(y_test, yPred)\n",
    "    prec = metrics.precision_score(y_test, yPred, average='micro')\n",
    "    rec = metrics.recall_score(y_test, yPred, average='micro')\n",
    "    f1 = metrics.f1_score(y_test, yPred, average='weighted', labels=np.unique(yPred))\n",
    "    roc_auc = metrics.roc_auc_score(y_test, yPred)\n",
    "    MCC = metrics.matthews_corrcoef(y_test, yPred)\n",
    "    matriz = confusion_matrix(y_test, yPred)\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "    espec = tn / (tn+fp)\n",
    "    return acc, prec, rec, f1, roc_auc, MCC, espec\n",
    "\n",
    "# Define los algoritmos de clasificación\n",
    "algs = [['Regresión Logística', LogisticRegression()],\n",
    "    ['Árbol de Decisión (Criterion Gini)', DecisionTreeClassifier(criterion='gini')],\n",
    "    ['Árbol de Decisión (Criterion Entropy)', DecisionTreeClassifier(criterion='entropy')],\n",
    "    ['Random Forest', RandomForestClassifier()],\n",
    "    ['Máquinas de Vectores de Soporte - Lineal', SVC(kernel='linear')],\n",
    "    ['Máquinas de Vectores de Soporte - RBF', SVC(kernel='rbf')]\n",
    "]\n",
    "\n",
    "\n",
    "def make_machine_learning(X, y, classifier, auto_enc = False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_dataset(X, y, random_state=1, test_size = 0.25)\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    yPred = classifier.predict(X_test)\n",
    "\n",
    "    measures = calculate_measures(y_test, yPred)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, measures, yPred\n",
    "\n",
    "def train(X, y):\n",
    "    fig = plt.figure(figsize =(26, 20))\n",
    "    fig.suptitle('Matrices de confusion', fontsize=30, fontweight='bold')\n",
    "    performance_measures = []   \n",
    "    i = 1\n",
    "    for name, classifier in algs:\n",
    "        tup = []\n",
    "        print('Entrenando modelo con la tecnica ' + name)\n",
    "        X_train, X_test, y_train, y_test, measures, yPred = make_machine_learning(X, y, classifier)\n",
    "        tup.append(name)\n",
    "        for mea in measures:\n",
    "            tup.append(mea)\n",
    "        performance_measures.append(tup)\n",
    "        print('Dibujando matrices de confusion para ' + name)\n",
    "        confusion_matrix = draw_matrix_confusion(y_test, yPred, i, name, measures)\n",
    "        i += 1\n",
    "    print('\\n')\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "    print('')\n",
    "    print(tabulate(performance_measures, headers=['Método', 'Exactitud', 'Precisión', 'Sensibilidad', 'Puntaje F1', 'Curva ROC AUC', 'MCC', 'Especificidad']))\n",
    "\n",
    "train(M_equi_x, M_equi_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(H_equi_x, H_equi_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
